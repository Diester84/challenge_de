{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Usando Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "# Lectura archivo JSON\n",
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "df = pd.read_json(file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se revisa el tipo de datos de algunas columnas\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se revisa formato de columna \"date\"\n",
    "print(df[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea funcion para aplanar columnas\n",
    "def aplanar_columnas(df, columnas):\n",
    "    for col in columnas:\n",
    "        col_df = pd.json_normalize(df[col])\n",
    "        # Se renombra columnas\n",
    "        col_df.columns = [f\"{col}_{subcol}\" for subcol in col_df.columns] \n",
    "        # Concatenacion con DF original\n",
    "        df = pd.concat([df.drop(col, axis=1), col_df], axis=1)\n",
    "    return df\n",
    "\n",
    "columnas = ['user']\n",
    "df = aplanar_columnas(df, columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se verifica que se haya creado las columnas correctamente\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se revisa columna a utilizar para analisis\n",
    "print(df['user_username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de formato fecha\n",
    "df['date'] = df['date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtiene el top 10 de días con más registros\n",
    "top_days= df.groupby('date').size().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea lista para resultado final\n",
    "top_usernames_per_day = []\n",
    "\n",
    "# Itera sobre cada uno de los top_days\n",
    "for day in top_days.index:\n",
    "    df_day = df[df['date'] == day]\n",
    "    # Cantidad de username repetidos para cada dia\n",
    "    username_counts = df_day['user_username'].value_counts()\n",
    "    # Encuentra el máximo valor\n",
    "    max_count = username_counts.max()\n",
    "    most_frequent_username = username_counts[username_counts == max_count].index[0]    \n",
    "    # Guarda el dia y el username en una lista\n",
    "    top_usernames_per_day.append((day, most_frequent_username))\n",
    "\n",
    "print(top_usernames_per_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "def q1_pandas(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    # Se crea funcion para aplanar columnas\n",
    "    def aplanar_columnas(df, columnas):\n",
    "        for col in columnas:\n",
    "            col_df = pd.json_normalize(df[col])\n",
    "            # Se renombra columnas\n",
    "            col_df.columns = [f\"{col}_{subcol}\" for subcol in col_df.columns] \n",
    "            # Concatenacion con DF original\n",
    "            df = pd.concat([df.drop(col, axis=1), col_df], axis=1)\n",
    "        return df\n",
    "\n",
    "    columnas = ['user']\n",
    "    df = aplanar_columnas(df, columnas)\n",
    "    # Transformación de formato fecha\n",
    "    df['date'] = df['date'].dt.date\n",
    "    # Se obtiene el top 10 de días con más registros\n",
    "    top_days= df.groupby('date').size().sort_values(ascending=False).head(10)\n",
    "    # Se crea lista para resultado final\n",
    "    top_usernames_per_day = []\n",
    "\n",
    "    # Itera sobre cada uno de los top_days\n",
    "    for day in top_days.index:\n",
    "        df_day = df[df['date'] == day]\n",
    "        # Cantidad de username repetidos para cada dia\n",
    "        username_counts = df_day['user_username'].value_counts()\n",
    "        # Encuentra el máximo valor\n",
    "        max_count = username_counts.max()\n",
    "        most_frequent_username = username_counts[username_counts == max_count].index[0]    \n",
    "        # Guarda el dia y el username en una lista\n",
    "        top_usernames_per_day.append((day, most_frequent_username))\n",
    "\n",
    "    # Imprime la lista de resultados\n",
    "    return(top_usernames_per_day)    \n",
    "\n",
    "# q1_pandas(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Usando PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, row_number\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar pyspark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "# Leer el archivo JSON\n",
    "df = spark.read.json(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se revisa columna user\n",
    "df.select(\"user\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se aplana columna user.username\n",
    "df = df.withColumn(\"user_username\", col(\"user.username\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se revisa columna date\n",
    "df.select(\"date\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se transforma para ver solo el dia\n",
    "df = df.withColumn(\"date\", to_date(col(\"date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rank, count, desc, max\n",
    "\n",
    "# Se agrupa por fecha y se cuentan los registros para cada dia\n",
    "date_counts = df.groupBy(\"date\").count()\n",
    "\n",
    "# Se buscan los top 10 dias con mas registros\n",
    "top_days = date_counts.orderBy(desc(\"count\")).limit(10)\n",
    "\n",
    "# Se une el dataFrame original con los top días para filtrar solo esos días\n",
    "df_top_days = df.join(top_days, \"date\")\n",
    "\n",
    "# Cuenta la cantidad de cada username por día\n",
    "username_counts = df_top_days.groupBy(\"date\", \"user_username\").count()\n",
    "\n",
    "# Busca el username con mayor cantidad de registros para cada dia\n",
    "max_username_count = username_counts.groupBy(\"date\").agg(max(\"count\").alias(\"max_count\"))\n",
    "\n",
    "# Genera un dataframe con la informacion requerida\n",
    "top_dates_usernames = username_counts.join(\n",
    "    max_username_count,\n",
    "    (username_counts[\"date\"] == max_username_count[\"date\"]) & \n",
    "    (username_counts[\"count\"] == max_username_count[\"max_count\"]),\n",
    "    \"right\"\n",
    ")\n",
    "\n",
    "# Join con toda la informacion que ordenar de mayor a menor cantidad de registros \n",
    "top_usernames_per_day= top_days.join(top_dates_usernames, \"date\", \"left\")\n",
    "\n",
    "# Recolecta los resultados como una lista de tuplas\n",
    "top_usernames_per_day_list = [(row['date'], row['user_username']) for row in top_usernames_per_day.collect()]\n",
    "\n",
    "print(top_usernames_per_day_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, rank, count, desc, max\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "def q1_pyspark(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    # Iniciar pyspark\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MyApp\") \\\n",
    "        .getOrCreate()\n",
    "    # Leer el archivo JSON\n",
    "    df = spark.read.json(file_path)\n",
    "    # Se aplana columna user.username\n",
    "    df = df.withColumn(\"user_username\", col(\"user.username\"))\n",
    "\n",
    "    # Se transforma para ver solo el dia\n",
    "    df = df.withColumn(\"date\", to_date(col(\"date\")))\n",
    "\n",
    "    # Se agrupa por fecha y se cuentan los registros para cada dia\n",
    "    date_counts = df.groupBy(\"date\").count()\n",
    "\n",
    "    # Se buscan los top 10 dias con mas registros\n",
    "    top_days = date_counts.orderBy(desc(\"count\")).limit(10)\n",
    "\n",
    "    # Se une el dataFrame original con los top días para filtrar solo esos días\n",
    "    df_top_days = df.join(top_days, \"date\")\n",
    "\n",
    "    # Cuenta la cantidad de cada username por día\n",
    "    username_counts = df_top_days.groupBy(\"date\", \"user_username\").count()\n",
    "\n",
    "    # Busca el username con mayor cantidad de registros para cada dia\n",
    "    max_username_count = username_counts.groupBy(\"date\").agg(max(\"count\").alias(\"max_count\"))\n",
    "\n",
    "    # Genera un dataframe con la informacion requerida\n",
    "    top_dates_usernames = username_counts.join(\n",
    "        max_username_count,\n",
    "        (username_counts[\"date\"] == max_username_count[\"date\"]) & \n",
    "        (username_counts[\"count\"] == max_username_count[\"max_count\"]),\n",
    "        \"right\"\n",
    "    )\n",
    "\n",
    "    # Join con toda la informacion que ordenar de mayor a menor cantidad de registros \n",
    "    top_usernames_per_day= top_days.join(top_dates_usernames, \"date\", \"left\")\n",
    "\n",
    "    # Recolecta los resultados como una lista de tuplas\n",
    "    top_usernames_per_day_list = [(row['date'], row['user_username']) for row in top_usernames_per_day.collect()]\n",
    "\n",
    "    return top_usernames_per_day_list\n",
    "\n",
    "# q1_pyspark(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tiempo de ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución: 9.113893270492554 segundos\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "q1_pandas(file_path)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Tiempo de ejecución: {end_time - start_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución: 11.244032382965088 segundos\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "q1_pyspark(file_path)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Tiempo de ejecución: {end_time - start_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que usando Pandas se obtiene un menor tiempo de ejecución (Pandas: 9.1 segundos, Pyspark: 11.24 segundos). Probablemente el tiempo de ejecución usando pandas podria disminuir mas si se optimiza el codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Uso de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in c:\\users\\diego\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.61.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\diego\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from memory_profiler) (5.9.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3009.22 MiB, increment: 2845.61 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit -r 1 q1_pandas(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 172.51 MiB, increment: 0.13 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit -r 1 q1_pyspark(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memoria Pandas: 3009.22 MiB, Memoria Pyspark: 172.51 MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtuvo que usando la libreria Pandas el tiempo de ejecución del código fue menor que usando la libreria PySpark, mientras que la memoria en uso fue menor utilizando PySpark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería Pandas almacena la información en la memoria ram de la maquina, mientras que la libreria PySpark puede utilizar tanto la memoria ram como el almacenamiento en disco para procesar datos, por lo que al usar Pandas hay mas memoria en uso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con respecto al tiempo de ejecución, pandas es mas eficiente que pyspark al trabajar con volumenes de datos que caben en la memoria de una maquina, lo cual se ve reflejado en una disminucion en el tiempo de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las preguntas 2 y 3, se considerará procesar los datos con Pandas para optimizar el tiempo de ejecución y procesar los datos con PySpark para optimizar la memoria en uso"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
